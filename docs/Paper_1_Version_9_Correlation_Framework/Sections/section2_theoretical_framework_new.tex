\section{Theoretical Framework}

We develop a mathematical framework for exploiting observed correlations in competitive measurement to achieve improved signal-to-noise ratios. The framework quantifies how positive correlations between competitors can be systematically leveraged through relative measurement approaches, building upon established principles in competitive measurement \cite{keiningham2015competitive}, statistical signal processing \cite{boll1979suppression}, and performance analysis \cite{hughes2002performance}.

\subsection{Correlation-Based Measurement Model}

Consider two competitors A and B with performance measurements exhibiting correlation structure. We model their observed performances as:

$$X_A = \mu_A + \epsilon_A, \quad X_B = \mu_B + \epsilon_B$$

where $\mu_A, \mu_B$ represent true performance capabilities, $\epsilon_A \sim \mathcal{N}(0, \sigma_A^2)$ and $\epsilon_B \sim \mathcal{N}(0, \sigma_B^2)$ represent competitor-specific variations, and $\text{Cov}(\epsilon_A, \epsilon_B) = \rho\sigma_A\sigma_B$ captures the correlation structure.

The key insight is that when $\rho > 0$, the relative measure $R = X_A - X_B$ achieves variance reduction:

$$\text{Var}(R) = \sigma_A^2 + \sigma_B^2 - 2\rho\sigma_A\sigma_B < \sigma_A^2 + \sigma_B^2$$

while preserving the signal of interest: $\mathbb{E}[R] = \mu_A - \mu_B$.

This correlation structure may arise from shared environmental conditions (weather, market factors, institutional effects) or other mechanisms. The framework's validity depends on the observable correlation structure rather than its underlying cause.

\subsection{Axiomatic Foundation}

We establish four fundamental axioms that define the necessary and sufficient conditions for effective correlation-based competitive measurement.

\begin{axiom}[Correlation-Based Variance Reduction]
For competitors A and B with correlation coefficient $\rho = \text{Cov}(X_A, X_B)/(\sigma_A \sigma_B)$, the relative measure $R = X_A - X_B$ achieves variance reduction when $\rho > 0$:

$$\text{Var}(R) = \sigma_A^2 + \sigma_B^2 - 2\rho\sigma_A\sigma_B < \sigma_A^2 + \sigma_B^2$$

\textbf{Testable Condition:} $\rho > 0$ must be empirically observable from paired competitor measurements.
\end{axiom}

\begin{axiom}[Signal Preservation]
The relative measure preserves the competitive signal while achieving variance reduction:

$$\mathbb{E}[R] = \mathbb{E}[X_A - X_B] = \mu_A - \mu_B$$

\textbf{Testable Condition:} The expected value of relative measurements must equal the true performance difference, ensuring competitive ordering is maintained.
\end{axiom}

\begin{axiom}[Scale Invariance]
For any positive scalar $\alpha > 0$, both the relative measure and correlation structure remain invariant under linear scaling:

$$R(\alpha X_A, \alpha X_B) = \alpha R(X_A, X_B)$$
$$\rho(\alpha X_A, \alpha X_B) = \rho(X_A, X_B)$$

\textbf{Testable Condition:} SNR improvement ratios must remain constant across different measurement scales when distributional properties are preserved.
\end{axiom}

\begin{axiom}[Statistical Optimality]
Under the correlation-based measurement model with regularity conditions (normality, finite variances, stable parameters), the relative measure $R = X_A - X_B$ is the minimum variance unbiased estimator (MVUE) of $\mu_A - \mu_B$ and achieves the CramÃ©r-Rao lower bound.

\textbf{Mathematical Statement:}
$$\text{Var}(R) = \sigma_A^2 + \sigma_B^2 - 2\rho\sigma_A\sigma_B = \text{CRLB}(\mu_A - \mu_B)$$

\textbf{Testable Condition:} No other unbiased estimator of the performance difference can achieve lower variance under the assumed measurement model.
\end{axiom}

\subsubsection{Axiom Completeness and Sufficiency}

\textbf{Necessity:} Each axiom establishes a required property for effective correlation-based competitive measurement:
\begin{itemize}
    \item Axiom 1 ensures the mechanism for improvement exists
    \item Axiom 2 ensures the competitive signal is preserved  
    \item Axiom 3 ensures universal applicability across scales
    \item Axiom 4 ensures theoretical optimality under model assumptions
\end{itemize}

\textbf{Sufficiency:} Together, the four axioms provide sufficient conditions for correlation-based competitive measurement to achieve predictable SNR improvements quantified by:

$$\text{SNR improvement ratio} = \frac{1 + \kappa}{1 + \kappa - 2\sqrt{\kappa}\rho}$$

where $\kappa = \sigma_B^2/\sigma_A^2$ and $\rho$ is the observed correlation coefficient.

\subsubsection{Empirical Validation Framework}

Each axiom provides specific testable predictions that guide empirical validation:

\begin{enumerate}
    \item \textbf{Correlation existence} ($\rho > 0$) can be directly measured from paired observations
    \item \textbf{Signal preservation} can be verified by comparing $\mathbb{E}[R]$ to known performance differences  
    \item \textbf{Scale invariance} can be tested by analyzing the same data at different scales
    \item \textbf{Statistical optimality} can be assessed by comparing relative measure efficiency to alternative estimators
\end{enumerate}

The axiomatic foundation thus provides both theoretical rigor and practical guidance for empirical validation of the correlation-based framework in specific application domains.

\subsection{Signal-to-Noise Ratio Improvement}

We define signal-to-noise ratios for absolute and relative measurement approaches:

\textbf{Absolute measurement (independent comparison):}
$$\text{SNR}_{\text{abs}} = \frac{(\mu_A - \mu_B)^2}{\sigma_A^2 + \sigma_B^2}$$

\textbf{Relative measurement (correlation exploitation):}
$$\text{SNR}_{\text{rel}} = \frac{(\mu_A - \mu_B)^2}{\sigma_A^2 + \sigma_B^2 - 2\rho\sigma_A\sigma_B}$$

The improvement ratio becomes:
$$\frac{\text{SNR}_{\text{rel}}}{\text{SNR}_{\text{abs}}} = \frac{\sigma_A^2 + \sigma_B^2}{\sigma_A^2 + \sigma_B^2 - 2\rho\sigma_A\sigma_B}$$

Introducing the variance ratio $\kappa = \sigma_B^2/\sigma_A^2$, this simplifies to:

$$\text{SNR improvement ratio} = \frac{1 + \kappa}{1 + \kappa - 2\sqrt{\kappa}\rho}$$

This formula exhibits complete scale independence: the $(\mu_A - \mu_B)^2$ terms cancel exactly, leaving improvement dependent only on distribution shape parameters $(\kappa, \rho)$.

\subsection{Framework Conditions and Limitations}

The framework requires several conditions for valid application:

\textbf{Distributional Requirements:}
\begin{itemize}
    \item Approximate normality (or transformable to normality)
    \item Stable variance relationships over measurement periods
    \item Meaningful correlation structure between competitors
\end{itemize}

\textbf{Measurement Requirements:}
\begin{itemize}
    \item Comparable measurement conditions between competitors
    \item Appropriate temporal alignment of measurements
    \item Positive correlation $\rho > 0$ between competitor performances
\end{itemize}

\textbf{Mathematical Constraints:}
\begin{itemize}
    \item Finite second moments for all measurements
    \item Stable parameter estimation across sample sizes
    \item Avoidance of critical region where $\kappa \approx 1$ and $\rho \approx 1$
\end{itemize}

\subsection{Parameter Space Analysis}

The improvement formula exhibits well-behaved mathematical properties across most parameter combinations:

\textbf{Enhancement Region ($\rho > 0$):} Relative measures outperform absolute measures with improvement proportional to correlation strength.

\textbf{Independence Region ($\rho = 0$):} Relative and absolute measures achieve equivalent performance.

\textbf{Degradation Region ($\rho < 0$):} Relative measures underperform absolute measures, though negative correlations are rarely observed in competitive contexts.

\textbf{Critical Point:} The formula approaches infinity as $(\kappa, \rho) \rightarrow (1, 1)$, representing the boundary where both competitors have identical variances and perfect positive correlation. This singular point is avoided in practical applications through safety constraints.

\subsection{Scale Independence and Cross-Domain Applicability}

The complete cancellation of signal magnitude terms ($\delta^2$) creates a scale-independent relationship:

$$\text{SNR improvement ratio} = f(\kappa, \rho) = \frac{1 + \kappa}{1 + \kappa - 2\sqrt{\kappa}\rho}$$

This property means that when the framework's assumptions are satisfied, the same mathematical relationship applies regardless of measurement units or absolute performance levels. However, each application domain requires empirical validation of:
\begin{itemize}
    \item Distributional assumptions (normality or transformability)
    \item Correlation structure stability
    \item Meaningful variance ratio calculations
\end{itemize}

The scale independence enables cross-domain comparison of framework effectiveness but does not guarantee universal applicability without assumption validation.

\subsection{Practical Implementation}

For practitioners applying the framework, the key steps involve:

\begin{enumerate}
    \item \textbf{Correlation Assessment:} Measure $\rho$ from paired observations of competitors
    \item \textbf{Variance Ratio Calculation:} Compute $\kappa = \sigma_B^2/\sigma_A^2$
    \item \textbf{Improvement Prediction:} Apply formula to predict SNR enhancement
    \item \textbf{Safety Verification:} Ensure distance from critical point $(\kappa=1, \rho=1)$
\end{enumerate}

The framework provides quantitative guidance for when relative measures offer advantages over traditional absolute approaches, with improvement magnitude determined by the observed correlation structure and variance asymmetry between competitors.

\subsection{Theoretical Foundation}

The mathematical framework builds upon established principles in statistical estimation and signal processing. Under the assumed measurement model, the relative measure $R = X_A - X_B$ provides an unbiased estimator of the performance difference $\mu_A - \mu_B$ with variance reduced by the correlation term.

The framework extends correlation-aware estimation techniques to competitive measurement contexts, providing a systematic approach to exploiting observed correlation structure for improved signal extraction. The mathematical foundation draws from information theory \cite{shannon1948mathematical} and statistical estimation theory, providing the theoretical basis for optimal competitive measurement design through correlation exploitation. While the theoretical optimality claims require standard regularity conditions (normality, independence of individual variations, stable parameters), the empirical effectiveness can be validated through direct performance comparison in specific application contexts.

\subsection{SNR-Prediction Performance Relationship}

A critical theoretical consideration is understanding how SNR improvements translate to enhanced binary prediction performance. The relationship between SNR and prediction accuracy is not linear and depends on the specific characteristics of the KPI in question.

\textbf{Theoretical Framework:}
Consider a binary prediction task where we predict competitive outcomes (win/loss) using performance measures. The prediction performance depends on the discriminative power of the measure, which is fundamentally related to the signal-to-noise ratio.

For a binary classifier using logistic regression:
$$P(\text{win}) = \text{logit}^{-1}(\beta_0 + \beta_1 X)$$

The discriminative power depends on the separation between the distributions of $X$ for winning and losing outcomes. This separation is quantified by the SNR, but the relationship is complex and KPI-dependent.

\textbf{SNR-Prediction Relationship:}
The relationship between SNR improvement and prediction performance follows:

$$\text{Prediction Improvement} \propto \frac{\Delta \text{SNR}}{\text{SNR}_{\text{baseline}}} \times f(\text{KPI characteristics})$$

where $f(\text{KPI characteristics})$ captures the KPI-specific factors that determine how SNR improvements translate to prediction gains.

\textbf{KPI-Specific Factors:}
The function $f(\text{KPI characteristics})$ depends on several factors:

\textbf{1. Distributional Properties:}
\begin{itemize}
    \item \textbf{Skewness:} Highly skewed KPIs may show different SNR-prediction relationships
    \item \textbf{Kurtosis:} Heavy-tailed distributions affect prediction boundary optimization
    \item \textbf{Outlier sensitivity:} KPIs with extreme values may have non-linear relationships
\end{itemize}

\textbf{2. Competitive Relevance:}
\begin{itemize}
    \item \textbf{Outcome correlation:} KPIs more strongly correlated with outcomes show better SNR-prediction relationships
    \item \textbf{Threshold effects:} Some KPIs may have non-linear relationships with competitive success
    \item \textbf{Context dependency:} The importance of a KPI may vary across different competitive contexts
\end{itemize}

\textbf{3. Measurement Characteristics:}
\begin{itemize}
    \item \textbf{Scale effects:} Different measurement scales affect how SNR improvements manifest
    \item \textbf{Precision:} Higher precision measurements show more consistent SNR-prediction relationships
    \item \textbf{Reliability:} More reliable KPIs demonstrate stronger SNR-prediction correlations
\end{itemize}

\textbf{Theoretical Predictions:}
Based on this framework, we can make several theoretical predictions:

\textbf{1. Non-Linear Relationship:}
The relationship between SNR improvement and prediction performance is not linear. Small SNR improvements may yield large prediction gains for some KPIs, while large SNR improvements may yield modest gains for others.

\textbf{2. KPI-Dependent Scaling:}
The scaling factor $f(\text{KPI characteristics})$ varies significantly across KPIs, explaining why some KPIs show dramatic prediction improvements while others show modest gains.

\textbf{3. Threshold Effects:}
There may exist SNR thresholds below which prediction improvements are minimal, and above which improvements accelerate.

\textbf{4. Diminishing Returns:}
For very high SNR values, additional improvements may yield diminishing returns in prediction performance.

\textbf{Mathematical Formulation:}
The theoretical relationship can be expressed as:

$$\text{AUC}_{\text{relative}} - \text{AUC}_{\text{absolute}} = \alpha \times \log\left(\frac{\text{SNR}_{\text{relative}}}{\text{SNR}_{\text{absolute}}}\right) \times f(\text{KPI})$$

where:
\begin{itemize}
    \item $\alpha$ is a scaling constant
    \item The logarithmic relationship captures the non-linear nature
    \item $f(\text{KPI})$ represents KPI-specific characteristics
\end{itemize}

\textbf{Empirical Validation Requirements:}
This theoretical framework makes several testable predictions:

\begin{enumerate}
    \item \textbf{Consistent Direction:} All KPIs should show improved prediction performance with higher SNR
    \item \textbf{Variable Magnitude:} The magnitude of improvement should vary significantly across KPIs
    \item \textbf{Statistical Significance:} Improvements should be statistically significant for most KPIs
    \item \textbf{Cross-Validation Stability:} Results should be stable across different validation folds
\end{enumerate}

This theoretical foundation provides the context for understanding why SNR improvements translate to prediction gains, while explaining the observed variability in improvement magnitudes across different KPIs.
